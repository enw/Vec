---
phase: 02-core-interface
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/llm/AnthropicClient.ts
  - src/llm/types.ts
  - src/llm/index.ts
autonomous: true

must_haves:
  truths:
    - "Streaming response delivers text chunks incrementally via callback"
    - "Cancelled stream preserves partial content with cancelled flag"
    - "API errors surface as structured error objects not crashes"
    - "Token usage reported after each response for budget tracking"
  artifacts:
    - path: "src/llm/AnthropicClient.ts"
      provides: "Streaming Anthropic API wrapper"
      exports: ["AnthropicClient"]
    - path: "src/llm/types.ts"
      provides: "StreamOptions, StreamResult, LLMMessage types"
      contains: "interface StreamOptions"
  key_links:
    - from: "src/llm/AnthropicClient.ts"
      to: "@anthropic-ai/sdk"
      via: "SDK client instantiation"
      pattern: "new Anthropic"
    - from: "src/llm/AnthropicClient.ts"
      to: "src/monitoring/tracker.ts"
      via: "Token usage reporting"
      pattern: "TokenUsageEvent"
---

<objective>
Build a pure TypeScript Anthropic streaming client (no React dependency) that wraps the SDK with cancellation, error handling, and token usage reporting.

Purpose: Separates LLM concerns from UI. This service is consumed by React hooks in Plan 04 but has zero React imports -- testable and reusable.
Output: AnthropicClient class with stream() method that delivers text chunks via callback, supports cancellation, handles errors gracefully, and reports token usage.
</objective>

<execution_context>
@/Users/enw/.claude/get-shit-done/workflows/execute-plan.md
@/Users/enw/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-core-interface/02-RESEARCH.md
@.planning/phases/02-core-interface/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: LLM types</name>
  <files>src/llm/types.ts</files>
  <action>
Create `src/llm/types.ts` with:

```typescript
import type { TokenUsageEvent } from '../types.js';

export interface LLMMessage {
  role: 'user' | 'assistant';
  content: string;
}

export interface StreamOptions {
  messages: LLMMessage[];
  model?: string;          // defaults to 'claude-sonnet-4-20250514'
  maxTokens?: number;      // defaults to 4096
  systemPrompt?: string;   // optional system message
  onChunk: (text: string) => void;  // called for each text delta
  signal?: AbortSignal;    // for cancellation
}

export interface StreamResult {
  content: string;         // full accumulated response
  cancelled: boolean;      // true if aborted mid-stream
  usage: TokenUsageEvent;  // token counts for budget tracking
  error?: string;          // error message if failed
}
```

Keep types simple. No over-abstraction for multi-provider (that's Phase 6).
  </action>
  <verify>`pnpm exec tsc --noEmit` passes.</verify>
  <done>LLM types defined: LLMMessage, StreamOptions, StreamResult.</done>
</task>

<task type="auto">
  <name>Task 2: AnthropicClient with streaming</name>
  <files>src/llm/AnthropicClient.ts, src/llm/index.ts</files>
  <action>
Create `src/llm/AnthropicClient.ts`:

Constructor: `new AnthropicClient(options?: { apiKey?: string })` -- if no apiKey, SDK reads ANTHROPIC_API_KEY from env automatically.

**`async stream(options: StreamOptions): Promise<StreamResult>`:**

1. Create Anthropic SDK client (reuse instance, create in constructor)
2. Build messages array for API (map LLMMessage[] to SDK format)
3. Call `client.messages.stream({ model, max_tokens, messages, system })` with options
4. Accumulate full response text in local variable
5. For each `text` event: append to accumulated text, call `options.onChunk(text)`
6. Handle cancellation: listen to `options.signal` abort event, call `stream.controller.abort()`. When aborted, set cancelled = true.
7. On completion: extract usage from `stream.finalMessage()` -- map to TokenUsageEvent format
8. On error: catch, return StreamResult with error field set, content = whatever was accumulated

Error handling specifics:
- `Anthropic.AuthenticationError`: Return error "Invalid API key. Set ANTHROPIC_API_KEY environment variable."
- `Anthropic.RateLimitError`: Return error "Rate limited. Please wait and try again."
- `Anthropic.APIError`: Return error with message from API
- Network errors: Return error "Network error. Check your connection."
- Abort errors (from cancellation): Not an error, return with cancelled = true

Token usage mapping:
```typescript
const finalMsg = await stream.finalMessage();
const usage: TokenUsageEvent = {
  model: options.model || 'claude-sonnet-4-20250514',
  promptTokens: finalMsg.usage.input_tokens,
  completionTokens: finalMsg.usage.output_tokens,
  totalTokens: finalMsg.usage.input_tokens + finalMsg.usage.output_tokens,
  timestamp: new Date().toISOString(),
};
```

Note: When stream is cancelled, `finalMessage()` may not resolve. In that case, construct partial usage with available info (set tokens to 0 if unknown).

Create `src/llm/index.ts` barrel export:
- Export AnthropicClient
- Export all types from types.ts
  </action>
  <verify>
`pnpm exec tsc --noEmit` passes. Can instantiate `new AnthropicClient()` without errors (API key not needed for instantiation).
  </verify>
  <done>AnthropicClient streams responses with chunk callbacks, cancellation support, error handling, and token usage reporting.</done>
</task>

</tasks>

<verification>
- `pnpm exec tsc --noEmit` -- zero errors
- AnthropicClient importable from src/llm/index.ts
- Types correctly model streaming lifecycle (start, chunks, complete/cancel/error)
- No React imports anywhere in src/llm/
</verification>

<success_criteria>
- Pure TS streaming client with zero React dependency
- Cancellation via AbortSignal works cleanly
- All Anthropic API errors handled with user-friendly messages
- Token usage extracted and returned as TokenUsageEvent
</success_criteria>

<output>
After completion, create `.planning/phases/02-core-interface/02-03-SUMMARY.md`
</output>
