---
phase: 03-memory-system
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/memory/TokenCounter.ts
  - src/memory/TokenAwareLoader.ts
  - src/memory/index.ts
autonomous: true

must_haves:
  truths:
    - "Token counter returns exact token count for message arrays"
    - "Message loader stops loading when approaching 100k token limit"
    - "Most recent messages are always included (load from newest backward)"
    - "Token counts are cached per message to avoid redundant API calls"
  artifacts:
    - path: "src/memory/TokenCounter.ts"
      provides: "Wrapper around Anthropic countTokens API with caching"
      exports: ["TokenCounter"]
    - path: "src/memory/TokenAwareLoader.ts"
      provides: "Loads conversation history within token budget"
      exports: ["TokenAwareLoader"]
  key_links:
    - from: "src/memory/TokenCounter.ts"
      to: "@anthropic-ai/sdk"
      via: "client.messages.countTokens()"
      pattern: "countTokens"
    - from: "src/memory/TokenAwareLoader.ts"
      to: "src/workspace/ConversationStore.ts"
      via: "store.loadAll() then filter by budget"
      pattern: "loadAll|TOKEN_LIMIT"
---

<objective>
Create token counting and token-aware message loading. The system counts tokens using Anthropic's official API, caches counts per message, and loads conversation history newest-first until hitting the 100k token budget.

Purpose: Thread memory must be bounded by tokens (user decision: hardcoded ~100k). This prevents context overflow and enables proactive summarization (Plan 04).
Output: TokenCounter class, TokenAwareLoader class.
</objective>

<execution_context>
@/Users/enw/.claude/get-shit-done/workflows/execute-plan.md
@/Users/enw/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-memory-system/03-RESEARCH.md
@src/workspace/ConversationStore.ts
@src/workspace/types.ts
@src/llm/AnthropicClient.ts
@src/llm/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: TokenCounter with caching</name>
  <files>src/memory/TokenCounter.ts</files>
  <action>
    Create `src/memory/TokenCounter.ts`:

    ```typescript
    import Anthropic from '@anthropic-ai/sdk';
    ```

    Class `TokenCounter`:
    - Constructor takes `{ client?: Anthropic; model?: string }` (defaults to creating new client, model 'claude-sonnet-4-20250514')
    - Private `cache: Map<string, number>` for message ID -> token count
    - `async countMessage(message: { id: string; role: string; content: string }): Promise<number>`
      - Check cache by message.id first, return if hit
      - Call `client.messages.countTokens({ model, messages: [{ role, content }] })`
      - Cache result by message.id
      - Return input_tokens
    - `async countMessages(messages: Array<{ id: string; role: string; content: string }>): Promise<number>`
      - Sum individual counts (sequential to avoid rate limits, but cached ones are instant)
      - Returns total
    - `async countSystemPrompt(systemPrompt: string): Promise<number>`
      - Calls countTokens with system param and empty message
      - Cache key: hash of systemPrompt content (use simple string hash)
    - `clearCache(): void` - clears the Map

    Important: Handle the case where Anthropic client is not available (no API key). In that case, fall back to char_count/4 estimate with a warning log. This allows offline/test usage.

    Export TOKEN_LIMIT = 100_000 and BUFFER_TOKENS = 10_000 as constants.
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - Unit test: mock Anthropic client, verify cache hit returns without API call
    - Unit test: verify fallback estimation when no API key
  </verify>
  <done>TokenCounter wraps Anthropic countTokens with per-message caching and offline fallback</done>
</task>

<task type="auto">
  <name>Task 2: TokenAwareLoader for bounded message loading</name>
  <files>
    src/memory/TokenAwareLoader.ts
    src/memory/index.ts
  </files>
  <action>
    Create `src/memory/TokenAwareLoader.ts`:

    Class `TokenAwareLoader`:
    - Constructor takes `{ counter: TokenCounter; store: ConversationStore }`
    - `async loadWithinBudget(options?: { systemPrompt?: string; reserveTokens?: number }): Promise<{ messages: Message[]; tokenCount: number; totalMessages: number; loadedMessages: number }>`
      - Load all messages from store.loadAll()
      - Calculate available budget: TOKEN_LIMIT - BUFFER_TOKENS - (systemPrompt tokens if provided) - (reserveTokens if provided)
      - Iterate from NEWEST to OLDEST (reverse order)
      - For each message, count tokens (via TokenCounter, which caches)
      - Accumulate until budget exceeded
      - Return messages in chronological order (re-reverse), plus metadata
    - `async shouldSummarize(): Promise<boolean>`
      - Load all messages, count total tokens
      - Return true if total > TOKEN_LIMIT * 0.8 (80% threshold per research)
    - `async getTokenStats(): Promise<{ totalTokens: number; messageCount: number; budgetUsed: number }>`
      - Returns current token usage stats for the conversation

    Update `src/memory/index.ts` to re-export TokenCounter, TokenAwareLoader, TOKEN_LIMIT, BUFFER_TOKENS.
  </action>
  <verify>
    - `npx tsc --noEmit` passes
    - Unit test: 10 messages, budget of 5 messages' worth -> returns last 5
    - Unit test: empty conversation returns empty array
    - Unit test: shouldSummarize returns true when >80% budget used
  </verify>
  <done>TokenAwareLoader loads most-recent messages within 100k token budget, reports when summarization needed</done>
</task>

</tasks>

<verification>
- TokenCounter caches per message ID, avoids redundant API calls
- TokenAwareLoader respects TOKEN_LIMIT (100k hardcoded per user decision)
- Loading prioritizes recent messages (newest first)
- Offline fallback works without API key (char/4 estimate)
- `pnpm build` succeeds
</verification>

<success_criteria>
- Token counting uses Anthropic's official countTokens API (not char/4 for production)
- Message loading bounded by hardcoded 100k token limit
- Recent messages always prioritized
- Cache prevents redundant API calls for same messages
- TypeScript compiles cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/03-memory-system/03-02-SUMMARY.md`
</output>
